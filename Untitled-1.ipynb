{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "s = HTMLSession()\n",
    "\n",
    "url = \"https://shopee.sg/search?keyword=samsung%20smartphone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(url):\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = getdata(url)\n",
    "\n",
    "# pulling all data sets on current page and verifying length\n",
    "containers = soup.findAll(\"div\", {\"class\": \"col-xs-2-4 shopee-search-item-result__item\"})\n",
    "len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of CSV Writing\n"
     ]
    }
   ],
   "source": [
    "soup = getdata(url)\n",
    "\n",
    "# pulling all data sets on current page and verifying length\n",
    "containers = soup.findAll(\"div\", {\"class\": \"qmXQo\"})\n",
    "# use below line to check the length of the dataset\n",
    "   # len(containers)\n",
    "   # containers\n",
    "   # filename = \"{}_Catalogue.csv\".format(url).replace(\"/\",\",\").replace(\"?\",\",\")\n",
    "   # f = open(filename, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "   # headers = \"Image_Url, Item_Name, Item_Price, Average_Rating, Number_Review\\n\"\n",
    "\n",
    "   # f.write(headers)\n",
    "\n",
    "filename = \"Lazada_Test_Catalogue.csv\"\n",
    "f = open(filename, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "headers = \"Image_Url, Item_Name, Item_Price, Number_Of_Ratings \\n\"\n",
    "\n",
    "f.write(headers)\n",
    "   # loop\n",
    "for container in containers:\n",
    "        Image_Url_Container = soup.find(\"div\", {\"class\": \"_95X4G\"})\n",
    "        Image_Url = Image_Url_Container[0].img[\"src\"]\n",
    "\n",
    "        Item_Name_Value = soup.find(\"div\", {\"class\": \"RfADt\"})\n",
    "        Item_Name = Item_Name_Value[0].text\n",
    "\n",
    "        Item_Price_Value = soup.find(\"span\", {\"class\": \"ooOxS\"})\n",
    "        Item_Price = Item_Price_Value[0].text\n",
    "\n",
    "        Number_Of_Ratings_Container = container.findAll(\n",
    "            \"span\", {\"class\": \"qzqFw\"})\n",
    "        Number_Of_Ratings = Number_Of_Ratings_Container[0].text\n",
    "\n",
    "        print(\"Image Url: \" + Image_Url)\n",
    "        print(\"Item_Name: \" + Item_Name)\n",
    "        print(\"Item_Price: \" + Item_Price)\n",
    "        print(\"Number_Of_Ratings: \" + Number_Of_Ratings)\n",
    "\n",
    "        f.write(Image_Url.replace(\",\", \"|\") + \",\" + Item_Name.replace(\",\", \"|\") + \",\" + Item_Price.replace(\",\", \".\")\n",
    "                + \",\" + Number_Of_Ratings.replace(\",\", \".\") + \"\\n\")\n",
    "\n",
    "f.close()\n",
    "print(\"End of CSV Writing\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbcb173fc87e39082b7a27eb521fb255480c0c279ccf10098ae0d92ed2e9a883"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
